import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import h5py
import json
from typing import List, Tuple, Dict, Any
import random
import time
import yaml
import argparse
from tqdm import tqdm

from prototypical_networks.prototype_models import (
    RobustSequenceEncoder, 
    robust_prototypical_loss,
    batch_hard_triplet_loss
)
from prototypical_networks.prototype_datasets import (
    RobustManeuverDataset, 
    custom_collate_fn,
    process_offline_data,
    process_target_data
)
from prototypical_networks.prototype_utils import (
    retrieve_maneuver_dtw,
    calibrate_dtw_thresholds
)

from benchmarking.benchmark_utils import process_retrieval_results

# --- Configuration Constants ---
FEATURE_SIZE = 512      # Output size of the learned embedding

# Training Parameters
K_SHOT = 7              # Number of support examples per class
N_QUERY = 3             # Number of query examples per class
N_EPISODES = 500        # Total training episodes

# --- Training Loop ---
def train_robust_network(encoder, maneuver_data):
    print("--- Starting Robust Euclidean Training ---")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    encoder.to(device)
    optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4)
    # Add a Learning Rate Scheduler
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)
    
    # Use the Robust Dataset with Time-Warping Augmentation
    dataset = RobustManeuverDataset(maneuver_data, N_WAY, K_SHOT, N_QUERY, N_EPISODES)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)
    
    encoder.train()
    total_loss = 0
    
    # 'batch_sequences' and 'mask' are generated by custom_collate_fn
    for i, (batch_sequences, mask) in enumerate(dataloader):
        optimizer.zero_grad()
        
        # Move to GPU
        batch_sequences = batch_sequences.to(device) # (B*Samples, L, Dim)
        mask = mask.to(device)                       # (B*Samples, L)
        
        # 3. Forward Pass with Masking
        # Mask ensures padding doesn't affect Global Max Pooling
        embeddings = encoder(batch_sequences, mask=mask)
        
        # 4. Compute Loss (Handles the Batch Dim automatically)
        loss = robust_prototypical_loss(embeddings, N_WAY, K_SHOT, N_QUERY)
        
        loss.backward()
        
        # Gradient clipping prevents spikes
        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        if i % 5 == 0:
            avg_loss = total_loss / (i + 1)
            print(f"Batch {i} | Episodes {i*4}/{N_EPISODES} | Avg Loss: {avg_loss:.4f}")
            
    return encoder

# --- Retrieval ---

def get_prototypes(encoder, reference_maneuvers):
    device = next(encoder.parameters()).device
    
    # --- 1. Calculate Temporal Prototypes (Variable Length) ---
    prototypes = {}
    print("Calculating Temporal Prototypes from variable-length data...")
    
    for class_id, sequences in reference_maneuvers.items():
        temporal_embeddings_list = []
        
        # Process each example individually to handle different lengths
        for seq_np in sequences[:K_SHOT]:
            with torch.no_grad():
                # (L, Dim) -> (1, L, Dim)
                seq_tensor = torch.from_numpy(seq_np).float().unsqueeze(0).to(device)
                # Get (1, L, Feature_Dim)
                features = encoder(seq_tensor, return_temporal=True).squeeze(0)
                temporal_embeddings_list.append(features)
        
        # To get the "Average Sequence" of different lengths, we interpolate 
        # them all to the MEDIAN length of the group before averaging.
        median_len = int(np.median([len(s) for s in temporal_embeddings_list]))
        
        resized_embeddings = []
        for feat in temporal_embeddings_list:
            # (L, Dim) -> (1, Dim, L) for interpolation
            feat_reshaped = feat.permute(1, 0).unsqueeze(0)
            resized = torch.nn.functional.interpolate(feat_reshaped, size=median_len, mode='linear', align_corners=True)
            resized_embeddings.append(resized.squeeze(0).permute(1, 0))
            
        # Now they are the same length, we can stack and mean
        prototypes[class_id] = torch.stack(resized_embeddings).mean(dim=0)
    return prototypes

def prototype_retrieval(encoder, prototypes, offline_data_list, output_path, episode_names, TOP_K=100):
    device = next(encoder.parameters()).device
    with h5py.File(output_path, 'w') as outfile:
        results = outfile.create_group('results')
        for class_id, proto_seq in prototypes.items():
            episode_key = episode_names[class_id]
            episode = results.create_group(episode_key)
            episode_results = []
            for offline_file in tqdm(offline_data_list, desc=f"Retrieving data for {episode_key}"):
                test_scenes = process_offline_data(offline_file)
                for demo_key, demo_data in test_scenes.items():                
                    with torch.no_grad():
                        scene_tensor = torch.from_numpy(demo_data).float().unsqueeze(0).to(device)
                        # Scenes are usually large, so this returns (Scene_L, Feature_Dim)
                        scene_features = encoder(scene_tensor, return_temporal=True).squeeze(0)
                    
                    # Numba DTW handles the different lengths of proto vs scene automatically
                    match = retrieve_maneuver_dtw(proto_seq.cpu().numpy(), scene_features.cpu().numpy())
                    episode_results.append({
                        'cost': match['cost'],
                        'start_idx': match['start_idx'],
                        'end_idx': match['end_idx'],
                        'demo_key': demo_key,
                        'offline_file': offline_file
                    })
            processed_results = process_retrieval_results(episode_results, top_k=TOP_K)
            for match_key, data in processed_results.items():
                match_group = episode.create_group(match_key)
                match_group.attrs['ep_meta'] = json.dumps({
                        "lang": data['lang_instruction']
                    })
                match_group.attrs['file_path'] = data['file_path']
                match_group.attrs['demo_key'] = data['demo_key']
                for data_key, value in data.items():
                    if data_key not in ['file_path', 'demo_key', 'lang_instruction']:
                        match_group.create_dataset(data_key, data=value)

if __name__ == '__main__':
    parser = argparse.ArgumentParser('Prototype-based Maneuver Retrieval')
    parser.add_argument('--config', type=str, default='config/config.yaml', help='Path to config file.')
    parser.add_argument('--dataset_type', default='droid', choices=['libero', 'nuscene', 'droid'], 
                       help='Type of dataset to use.')
    parser.add_argument('--pretrained', default=False, action='store_true', help='Use pretrained model for retrieval.')
    args = parser.parse_args()
    config = yaml.safe_load(open(args.config, 'r'))

    if args.dataset_type == 'libero':
        target_data = config['dataset_paths']['libero_target']
        offline_data_dir = config['dataset_paths']['libero_offline']
        retrieved_output = os.path.join(config['retrieval_paths'], 'libero_retrieval_results_prototype.hdf5')
        checkpoint_name = config['prototype_ckpt_paths']['libero']
    elif args.dataset_type == 'nuscene':
        target_data = config['dataset_paths']['nuscene_target']
        offline_data_dir = config['dataset_paths']['nuscene_offline']
        retrieved_output = os.path.join(config['retrieval_paths'], 'nuscene_retrieval_results_prototype.hdf5')
        checkpoint_name = config['prototype_ckpt_paths']['nuscene']
    elif args.dataset_type == 'droid':
        target_data = config['dataset_paths']['droid_target']
        offline_data_dir = config['dataset_paths']['droid_offline']
        retrieved_output = os.path.join(config['retrieval_paths'], 'droid_retrieval_results_prototype.hdf5')
        checkpoint_name = config['prototype_ckpt_paths']['droid']
    else:
        raise ValueError("Unsupported dataset type!")

    os.makedirs(os.path.dirname(retrieved_output), exist_ok=True)    
    reference_maneuver_data, episode_names = process_target_data(target_data)
    N_DIM =reference_maneuver_data[0][0].shape[-1]
    N_CLASSES = len(reference_maneuver_data)
    N_WAY = N_CLASSES//2  # Update N_WAY based on number of classes
    
    encoder = RobustSequenceEncoder(input_dim=N_DIM, output_dim=FEATURE_SIZE)

    if not args.pretrained:
        start_time = time.time()
        encoder = train_robust_network(encoder, reference_maneuver_data)
        training_time = time.time() - start_time
        print(f"Total training time: {training_time:.2f} seconds.")
    
        # Save the trained model
        torch.save(encoder.state_dict(), checkpoint_name)
    else:
        # load the trained model (for inference)
        if not os.path.exists(checkpoint_name):
            raise ValueError("Checkpoint file not found!! Train the model first.")
        encoder.load_state_dict(torch.load(checkpoint_name))
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        encoder.to(device)

    encoder.eval()
    prototypes = get_prototypes(encoder, reference_maneuver_data)
    #episode_thresholds = calibrate_dtw_thresholds(encoder, prototypes, reference_maneuver_data, K_SHOT, std_mult=1.0)

    # C. Retrieval
    offline_data_list = glob.glob(offline_data_dir)
    prototype_retrieval(encoder, prototypes, offline_data_list, retrieved_output, episode_names, TOP_K=100)