import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import h5py
import json
from typing import List, Tuple, Dict, Any
import random
import time
import argparse
from tqdm import tqdm

from prototypical_networks.prototype_models import (
    RobustSequenceEncoder, 
    robust_prototypical_loss
)
from prototypical_networks.prototype_datasets import (
    RobustManeuverDataset, 
    custom_collate_fn,
    process_offline_data,
    process_target_data
)
from prototypical_networks.prototype_utils import (
    retrieve_maneuver_dtw,
    calibrate_dtw_thresholds
)
# --- Configuration Constants ---
N_DIM = 12              # Number of features
FEATURE_SIZE = 512      # Output size of the learned embedding
N_CLASSES = 9           # Number of maneuver types

# Training Parameters
N_WAY = 5               # Number of classes per episode
K_SHOT = 7              # Number of support examples per class
N_QUERY = 3             # Number of query examples per class
N_EPISODES = 500        # Total training episodes

# --- Dataset ---

def process_target_data(target_data_file):
    """Simulates realistic, variable-length maneuver data."""
    maneuvers = {}
    class_names ={}
    with h5py.File(target_data_file, 'r') as f:
        for i, task in enumerate(f.keys()):
            data = f[task]

            joint_states = data['joint_states'][:]
            gripper_states = data['gripper_states'][:]
            ee_pos = data['ee_pos'][:]
            all_data = np.concatenate([joint_states, gripper_states, ee_pos], axis=2)
            # Convert to list of 2D arrays for sampling
            maneuvers[i] = [all_data[j] for j in range(all_data.shape[0])]
            class_names[i] = task

    return maneuvers, class_names

def process_offline_data(offline_data):
    target_segments_list = {}  
    with h5py.File(offline_data, 'r') as f:
        data = f['data']
        demo_list = list(data.keys())
        maneuver = [[] for _ in range(N_CLASSES)]
        for demo in demo_list:
            joint_states = data[demo]['obs/joint_states'][:]
            gripper_states = data[demo]['obs/gripper_states'][:]
            ee_pos = data[demo]['obs/ee_pos'][:]
            states = data[demo]['states'][:]
            #states = reduce_features_pca(states, n_components=N_DIM)
            all_data = np.concatenate([joint_states, gripper_states, ee_pos], axis=1)
            target_segments_list[demo] = all_data.astype(np.float32)
    return target_segments_list

# --- Training Loop ---
def train_robust_network(encoder, maneuver_data):
    print("--- Starting Robust Euclidean Training ---")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    encoder.to(device)
    optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4)
    # Add a Learning Rate Scheduler
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)
    
    # Use the Robust Dataset with Time-Warping Augmentation
    dataset = RobustManeuverDataset(maneuver_data, N_WAY, K_SHOT, N_QUERY, N_EPISODES)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)
    
    encoder.train()
    total_loss = 0
    
    # 'batch_sequences' and 'mask' are generated by custom_collate_fn
    for i, (batch_sequences, mask) in enumerate(dataloader):
        optimizer.zero_grad()
        
        # Move to GPU
        batch_sequences = batch_sequences.to(device) # (B*Samples, L, Dim)
        mask = mask.to(device)                       # (B*Samples, L)
        
        # 3. Forward Pass with Masking
        # Mask ensures padding doesn't affect Global Max Pooling
        embeddings = encoder(batch_sequences, mask=mask)
        
        # 4. Compute Loss (Handles the Batch Dim automatically)
        loss = robust_prototypical_loss(embeddings, N_WAY, K_SHOT, N_QUERY)
        
        loss.backward()
        
        # Gradient clipping prevents spikes
        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        if i % 5 == 0:
            avg_loss = total_loss / (i + 1)
            print(f"Batch {i} | Episodes {i*4}/{N_EPISODES} | Avg Loss: {avg_loss:.4f}")
            
    return encoder

# --- Retrieval ---

def get_prototypes(encoder, reference_maneuvers):
    device = next(encoder.parameters()).device
    
    # --- 1. Calculate Temporal Prototypes (Variable Length) ---
    prototypes = {}
    print("Calculating Temporal Prototypes from variable-length data...")
    
    for class_id, sequences in reference_maneuvers.items():
        temporal_embeddings_list = []
        
        # Process each example individually to handle different lengths
        for seq_np in sequences[:K_SHOT]:
            with torch.no_grad():
                # (L, Dim) -> (1, L, Dim)
                seq_tensor = torch.from_numpy(seq_np).float().unsqueeze(0).to(device)
                # Get (1, L, Feature_Dim)
                features = encoder(seq_tensor, return_temporal=True).squeeze(0)
                temporal_embeddings_list.append(features)
        
        # To get the "Average Sequence" of different lengths, we interpolate 
        # them all to the MEDIAN length of the group before averaging.
        median_len = int(np.median([len(s) for s in temporal_embeddings_list]))
        
        resized_embeddings = []
        for feat in temporal_embeddings_list:
            # (L, Dim) -> (1, Dim, L) for interpolation
            feat_reshaped = feat.permute(1, 0).unsqueeze(0)
            resized = torch.nn.functional.interpolate(feat_reshaped, size=median_len, mode='linear', align_corners=True)
            resized_embeddings.append(resized.squeeze(0).permute(1, 0))
            
        # Now they are the same length, we can stack and mean
        prototypes[class_id] = torch.stack(resized_embeddings).mean(dim=0)
    return prototypes

def retrieve(encoder, prototypes, offline_data_list, class_names, thresholds):
    device = next(encoder.parameters()).device
    all_results = dict.fromkeys(class_names.values(), [])
    task_results = [[] for _ in range(N_CLASSES)]

    for offline_data in tqdm(offline_data_list, desc="Processing Offline Data"):
        task_name = os.path.splitext(os.path.basename(offline_data))[0]
        test_scenes = process_offline_data(offline_data)

        for class_id, proto_seq in prototypes.items():
            class_threshold = thresholds[class_id]
            demo_results = []
            for scene_name, scene_data in test_scenes.items():                
                with torch.no_grad():
                    scene_tensor = torch.from_numpy(scene_data).float().unsqueeze(0).to(device)
                    # Scenes are usually large, so this returns (Scene_L, Feature_Dim)
                    scene_features = encoder(scene_tensor, return_temporal=True).squeeze(0)
                
                # Numba DTW handles the different lengths of proto vs scene automatically
                match = retrieve_maneuver_dtw(proto_seq.cpu().numpy(), scene_features.cpu().numpy())
                match['scene'] = scene_name
                match['task'] = task_name
                # TODO: Tune this threshold based on validation set or target set
                if match['score'] < class_threshold:
                    demo_results.append(match)
            # Filter overlaps
            task_results[class_id].extend(demo_results)
    
    for class_id in range(N_CLASSES):
        task_results[class_id] = sorted(task_results[class_id], key=lambda x: x['score'])
        all_results[class_names[class_id]] = task_results[class_id]

    return all_results
# --- Main Execution ---

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Prototype Libero Retrieval')
    parser.add_argument('--reference_data', type=str, default='data/retrieval_results/target_dataset.hdf5',
                        help='Path to the reference maneuver data HDF5 file')
    parser.add_argument('--offline_data_dir', type=str, default='data/LIBERO/libero_90/*demo.hdf5',
                        help='Path pattern to the offline data HDF5 files')
    parser.add_argument('--pretrained', action='store_true', default=True,
                        help='Use pretrained model for retrieval without training')
    
    args = parser.parse_args()
    
    reference_maneuver_data, class_names = process_target_data(args.reference_data)
    encoder = RobustSequenceEncoder(input_dim=N_DIM, output_dim=FEATURE_SIZE)

    if not args.pretrained:
        start_time = time.time()
        encoder = train_robust_network(encoder, reference_maneuver_data)
        training_time = time.time() - start_time
        print(f"Total training time: {training_time:.2f} seconds.")
    
        # Save the trained model
        torch.save(encoder.state_dict(), f'prototype_libero_model_{N_DIM}.pth')

    else:
        # load the trained model (for inference)
        encoder.load_state_dict(torch.load(f'prototype_libero_model_{N_DIM}.pth'))
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        encoder.to(device)

    encoder.eval()
    prototypes = get_prototypes(encoder, reference_maneuver_data)
    class_thresholds = calibrate_dtw_thresholds(encoder, prototypes, reference_maneuver_data, K_SHOT)

    # C. Retrieval
    final_results = {}
    offline_data_list = glob.glob(args.offline_data_dir)
    
    final_results = retrieve(encoder, prototypes, offline_data_list, class_names, class_thresholds)

    with open(f'prototype_results_{N_DIM}.json', 'w') as f:
        json.dump(final_results, f, indent=4)